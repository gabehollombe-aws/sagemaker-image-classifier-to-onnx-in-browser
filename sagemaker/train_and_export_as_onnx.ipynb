{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some variables for the local directories we'll work in for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='/tmp'\n",
    "dataset_name = 'caltech_objects'\n",
    "dataset_dir = base_dir + '/' + dataset_name\n",
    "\n",
    "%env BASE_DIR=$base_dir\n",
    "%env DATASET_DIR=$dataset_dir\n",
    "%env DATASET_NAME=$dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab a bunch of images grouped by folders, one per label class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf $DATASET_DIR\n",
    "mkdir -p $DATASET_DIR\n",
    "cd $DATASET_DIR\n",
    "wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz --quiet\n",
    "tar -xzf 101_ObjectCategories.tar.gz\n",
    "mv 101_ObjectCategories/* .\n",
    "rm -rf 101_ObjectCategories\n",
    "rm 101_ObjectCategories.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what classes of data we have from our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $DATASET_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at a few example images from some of our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Image, DisplayObject\n",
    "\n",
    "def show_thumbnails(path, num):\n",
    "    names = [f for f in os.listdir(path) if f.endswith('.png') or f.endswith('jpg')]\n",
    "    for name in names[:num]:\n",
    "        display(Image(filename=path + '/' + name, width=100))\n",
    "        \n",
    "show_thumbnails(dataset_dir + '/Faces', 4)\n",
    "show_thumbnails(dataset_dir + '/garfield', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some variables for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find im2rec in our environment and set up some other vars in our environemnt\n",
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter( (lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "%env IM2REC=$im2rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to 224x224\n",
    "# Sometimes, our input images aren't already in the desired format for training. I like to format all my images to be the correct size before I train my model.\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def resize(filepath, size=224, fill_color=(0, 0, 0, 0)):\n",
    "    try:\n",
    "        image = Image.open(filepath)\n",
    "    except:\n",
    "        os.remove(filepath)\n",
    "        return\n",
    "\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    image.thumbnail([size, size])\n",
    "    (w, h) = image.size\n",
    "    new_im.paste(image, (int((size - w) / 2), int((size - h) / 2 )))\n",
    "\n",
    "    # Overwrite file with resized version\n",
    "    new_im.save(filepath, \"JPEG\", quality = 95)\n",
    "\n",
    "def recursively_process_files(dirname, processing_func):\n",
    "    for dirname, dirnames, filenames in os.walk(dirname):\n",
    "        for subdirname in dirnames:\n",
    "            recursively_process_files(subdirname, processing_func)\n",
    " \n",
    "        for filename in filenames:\n",
    "            processing_func(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "recursively_process_files(f\"{dataset_dir}\", resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Use the IM2REC script to convert our images into RecordIO files\n",
    "\n",
    "cd $BASE_DIR\n",
    "\n",
    "rm ${DATASET_NAME}_classes\n",
    "\n",
    "# First we need to create two LST files (training and test lists), noting the correct label class for each image\n",
    "# We'll also save the output of the LST files command, since it includes a list of all of our label classes\n",
    "echo \"Creating LST files\"\n",
    "python $IM2REC --list --recursive --pass-through --test-ratio=0.3 --train-ratio=0.7 $DATASET_NAME $DATASET_NAME > ${DATASET_NAME}_classes\n",
    "\n",
    "# Then we create RecordIO files from the LST files\n",
    "echo \"Creating RecordIO files\"\n",
    "rm *.rec\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_train.lst $DATASET_NAME\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_test.lst $DATASET_NAME\n",
    "ls -lh *.rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of class labels into a python variable to use later\n",
    "classes_file_name = dataset_name + '_classes'\n",
    "path = base_dir + '/' + classes_file_name\n",
    "class_labels = ! cut -d \" \" -f1 {path} | paste -sd \" \"\n",
    "class_labels = class_labels[0].split(' ')\n",
    "\n",
    "# Show a few of them now just so we can see the kind of labels we're working with...\n",
    "class_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload our train and test RecordIO files to S3 in the bucket that our sagemaker session is using\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "s3train_path = 's3://{}/{}/train/'.format(bucket, dataset_name)\n",
    "s3validation_path = 's3://{}/{}/validation/'.format(bucket, dataset_name)\n",
    "\n",
    "# Clean up any existing data in our training s3 bucket\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/train --recursive\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/validation --recursive\n",
    "\n",
    "# Upload the rec files to the train and validation channels\n",
    "!aws s3 cp /tmp/{dataset_name}_train.rec $s3train_path\n",
    "!aws s3 cp /tmp/{dataset_name}_test.rec $s3validation_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the data for our model training to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3train_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3validation_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, dataset_name)\n",
    "image_classifier = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_wait=60*60,\n",
    "    train_max_run=60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=! ls -l {base_dir}/{dataset_name} | wc -l\n",
    "num_classes=int(num_classes[0]) - 1\n",
    "\n",
    "num_training_samples=! cat {base_dir}/{dataset_name}_train.lst | wc -l\n",
    "num_training_samples = int(num_training_samples[0])\n",
    "\n",
    "# Learn more about the Sagemaker built-in Image Classifier hyperparameters here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "# These hyperparameters we won't want to change, as they define things like\n",
    "# the size of the images we'll be sending for input, the number of training classes we have, etc.\n",
    "base_hyperparameters=dict(\n",
    "    use_pretrained_model=1,\n",
    "    image_shape='3,224,224',\n",
    "    num_classes=num_classes,\n",
    "    num_training_samples=num_training_samples,\n",
    ")\n",
    "\n",
    "# These are hyperparameters we may want to tune, as they can affect the model training success:\n",
    "hyperparameters={\n",
    "    **base_hyperparameters, \n",
    "    **dict(\n",
    "        epochs=60,\n",
    "        learning_rate=0.001,\n",
    "        mini_batch_size=64,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "image_classifier.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "This will take some time because it's provisioning a new container runtime to train our model, then the actual training happens, then the trained model gets uploaded to S3 and the container is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_classifier.fit(inputs=data_channels, logs=True)\n",
    "\n",
    "job = image_classifier.latest_training_job\n",
    "model_path = f\"{base_dir}/{job.name}\"\n",
    "\n",
    "print(f\"\\n\\n Finished training! The model is available for download at: {image_classifier.output_path}/{job.name}/output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting our SageMaker-trained model to the ONNX Format\n",
    "\n",
    "SageMaker uses a framework called MXNet to train and produce our image classifier model.  But, we might want to use this model to perform inference somehwere that MXNet doesn't easily run, such as in a web browser.  \n",
    "\n",
    "ONNX is an open format to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools and choose the combination that is best for them. More info at https://onnx.ai/\n",
    "\n",
    "SageMaker provides helpful tooling in the Python SDK to convert trained models to the ONNX format, making it easy to take your trained model, convert it to ONNX, then use that model in whatever environment you want to (as long as that environment will accept models in the ONNX format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path where our trained model was saved to S3\n",
    "model_s3_path = image_classifier.model_data\n",
    "model_s3_output_dir = \"/\".join(model_s3_path.split('/')[0:-1])\n",
    "%env MODEL_S3_PATH = $model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download our model and extract it into $BASE_DIR/downloaded_model\n",
    "rm -rf $BASE_DIR/downloaded_model/\n",
    "mkdir -p $BASE_DIR/downloaded_model/\n",
    "aws s3 cp $MODEL_S3_PATH $BASE_DIR/downloaded_model/model.tar.gz\n",
    "cd $BASE_DIR/downloaded_model\n",
    "tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MXNet's onnx_mxnet module to convert the MXNet model that SageMaker trained into ONNX format\n",
    "from mxnet.contrib import onnx as onnx_mxnet\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "model_dir = base_dir + \"/downloaded_model\"\n",
    "\n",
    "model_symbol_file = glob(model_dir + \"/*symbol.json\")[0]\n",
    "model_params_file = glob(model_dir + \"/*.params\")[0]\n",
    "\n",
    "onnx_mxnet.export_model(sym=model_symbol_file,\n",
    "                            params=model_params_file,\n",
    "                            input_shape=[(1, 3, 224, 224)],\n",
    "                            input_type=np.float32,\n",
    "                            onnx_file_path=\"{}/model.onnx\".format(model_dir),\n",
    "                            verbose=True)\n",
    "\n",
    "# And upload the ONNX model back to the same place on S3 as where we Sagemaker put the MXNet version of the model, just for safe-keeping\n",
    "! aws s3 cp {model_dir}/model.onnx {model_s3_output_dir}/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, display a link so we can download the ONNX version of the model easily from this notebook's local disk\n",
    "\n",
    "from IPython.display import FileLink\n",
    "# DownloadFileLink via https://github.com/jupyterlab/jupyterlab/issues/5443\n",
    "class DownloadFileLink(FileLink):\n",
    "    html_link_str = \"<a href='{link}' download={file_name}>{link_text}</a>\"\n",
    "\n",
    "    def __init__(self, path, file_name=None, link_text=None, *args, **kwargs):\n",
    "        super(DownloadFileLink, self).__init__(path, *args, **kwargs)\n",
    "\n",
    "        self.file_name = file_name or os.path.split(path)[1]\n",
    "        self.link_text = link_text or self.file_name\n",
    "\n",
    "    def _format_path(self):\n",
    "        from html import escape\n",
    "        fp = ''.join([self.url_prefix, escape(self.path)])\n",
    "        return ''.join([self.result_html_prefix,\n",
    "                        self.html_link_str.format(link=fp, file_name=self.file_name, link_text=self.link_text),\n",
    "                        self.result_html_suffix])\n",
    "    \n",
    "# We'll need to symlink the onnx model file from base_dir/downloaded_model/model.onnx to this notebook's home directory \n",
    "# so that Jupyter will serve it\n",
    "! ln -fs {model_dir}/model.onnx ./model.onnx\n",
    "\n",
    "# Output the download link for us to click on\n",
    "DownloadFileLink(\"model.onnx\", result_html_prefix=\"Click here to download the model in ONNX format: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we'll also want the handy list of knowing all the classes that our model returns scores for. \n",
    "# Our model returns a score for each label, in this same order.\n",
    "\n",
    "print(' '.join(class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ONNX Model for inference\n",
    "\n",
    "Now that you have an ONNX format of your image classifer model downloaded, \n",
    "you can use that model, along with the list of class labels (shown above),\n",
    "to make inferences about images any place that can run ONNX models, including \n",
    "offline, in your web browser! \n",
    "\n",
    "Want to give that a try? Download your ONNX model, copy the class labels list above,\n",
    "then use the single-page JavaScript app linked below which will load your model into your web browser (it doesn't upload the model anywhere)\n",
    "and make inferences on images that you drag and drop onto the page.\n",
    "\n",
    "Local ONNX image classifer inference in the browser frontend. \n",
    "\n",
    "Clone on GitHub: https://github.com/gabehollombe-aws/sagemaker-image-classifier-to-onnx-in-browser\n",
    "\n",
    "Edit/Run on Glitch: https://glitch.com/~gabehollombe-aws-sagemaker-image-classifier-to-onnx-in-browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Using SageMaker to host a deployed version of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Deploying a model to an endpoint takes a few minutes to complete\n",
    "\n",
    "# ---------------\n",
    "\n",
    "assert False # Change to True to run this\n",
    "\n",
    "# ---------------\n",
    "import time\n",
    "now = str(int(time.time()))\n",
    "\n",
    "deployed_endpoint = image_classifier.deploy(\n",
    "    endpoint_name = dataset_name.replace('_', '-') + '-' + now,\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.t2.medium',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling a deployed endpoint from Python code\n",
    "\n",
    "Once you've deployed to a SakeMaker hosted endpoint, you'll want to pass it some images to see it perform inferences.  Here's how to do this from some python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def classify_deployed(file_name, classes):\n",
    "    payload = None\n",
    "    with open(file_name, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    deployed_endpoint.content_type = 'application/x-image'\n",
    "    result = json.loads(deployed_endpoint.predict(payload))\n",
    "    print(result)\n",
    "    best_prob_index = np.argmax(result)\n",
    "    return (classes[best_prob_index], result[best_prob_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "\n",
    "assert False # Change to True to run this\n",
    "\n",
    "# ---------------\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_url=\"https://some/image/to/download\"\n",
    "\n",
    "# Download an image from a URL\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# And resize it\n",
    "size = 224\n",
    "new_im = Image.new('RGB', (size, size), (0, 0, 0, 0))\n",
    "img.thumbnail([size, size])\n",
    "(w, h) = img.size\n",
    "new_im.paste(img, (int((size - w) / 2), int((size - h) / 2 )))\n",
    "new_im.save(tmp_local_path, \"JPEG\", quality = 95)\n",
    "\n",
    "# Show it\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=tmp_local_path))\n",
    "\n",
    "# Classify it\n",
    "classify_deployed(tmp_local_path, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Perform Hyperparameter Tuning\n",
    "\n",
    "Often, you might not know which values for hyperparameters like `learning_rate` and `mini_batch_size` will yield acceptible results. Traditionally, this meant manually running many training jobs with different hyperparameter values, looking at each trained model's performance, and then picking a winner. \n",
    "\n",
    "\n",
    "This type of manual tuning is _very_ time consuming, so you can automate this process using automatic model tuning with SageMaker. Here's some example code to illustrate how to start one of these jobs using the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "\n",
    "assert False # Change to True to run this\n",
    "\n",
    "# ---------------\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "hyperparameter_ranges = {\n",
    "    'optimizer': CategoricalParameter(['sgd', 'adam', 'rmsprop', 'nag']),\n",
    "     'learning_rate': ContinuousParameter(0.0001, 0.1),\n",
    "     'mini_batch_size': CategoricalParameter([4, 8, 16, 32]),\n",
    "     'momentum': ContinuousParameter(0.0, 0.99),                                                                   \n",
    "     'weight_decay': ContinuousParameter(0.0, 0.99),   \n",
    "}\n",
    "\n",
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    image_classifier,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    early_stopping_type='Off',\n",
    "    max_jobs=500,\n",
    "    max_parallel_jobs=2\n",
    ")\n",
    "\n",
    "\n",
    "tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "When you're done learning here, you'll likley want to delete the endpoint you deployed (if you did so), since that endpoint will charge you money even if you're not actively making inferences (because you're charged for the total availability time of the endpoint). Here's how to clean up the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "\n",
    "assert False # Change to True to run this\n",
    "\n",
    "# ---------------\n",
    "deployed_endpoint.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}