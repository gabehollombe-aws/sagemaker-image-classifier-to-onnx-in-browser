{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some variables for the local directories we'll work in for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=/tmp\n",
      "env: DATASET_DIR=/tmp/caltech_objects\n",
      "env: DATASET_NAME=caltech_objects\n"
     ]
    }
   ],
   "source": [
    "base_dir='/tmp'\n",
    "dataset_name = 'caltech_objects'\n",
    "dataset_dir = base_dir + '/' + dataset_name\n",
    "\n",
    "%env BASE_DIR=$base_dir\n",
    "%env DATASET_DIR=$dataset_dir\n",
    "%env DATASET_NAME=$dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab a bunch of images grouped by folders, one per label class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf $DATASET_DIR\n",
    "mkdir -p $DATASET_DIR\n",
    "cd $DATASET_DIR\n",
    "wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz --quiet\n",
    "tar -xzf 101_ObjectCategories.tar.gz\n",
    "mv 101_ObjectCategories/* .\n",
    "rm -rf 101_ObjectCategories\n",
    "rm 101_ObjectCategories.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some variables for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: IM2REC=/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/tools/im2rec.py\n"
     ]
    }
   ],
   "source": [
    "# Find im2rec in our environment and set up some other vars in our environemnt\n",
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter( (lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "%env IM2REC=$im2rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Resize images to 224x224\n",
    "# Sometimes, our input images aren't already in the desired format for training. I like to format all my images to be the correct size before I train my model.\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def resize(filepath, size=224, fill_color=(0, 0, 0, 0)):\n",
    "    try:\n",
    "        image = Image.open(filepath)\n",
    "    except:\n",
    "        os.remove(filepath)\n",
    "        return\n",
    "\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    image.thumbnail([size, size])\n",
    "    (w, h) = image.size\n",
    "    new_im.paste(image, (int((size - w) / 2), int((size - h) / 2 )))\n",
    "\n",
    "    # Overwrite file with resized version\n",
    "    new_im.save(filepath, \"JPEG\", quality = 95)\n",
    "\n",
    "def recursively_process_files(dirname, processing_func):\n",
    "    for dirname, dirnames, filenames in os.walk(dirname):\n",
    "        for subdirname in dirnames:\n",
    "            recursively_process_files(subdirname, processing_func)\n",
    " \n",
    "        for filename in filenames:\n",
    "            processing_func(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "recursively_process_files(f\"{dataset_dir}\", resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LST files\n",
      "Creating RecordIO files\n",
      "Creating .rec file from /tmp/caltech_objects_train.lst in /tmp\n",
      "time: 0.031055688858032227  count: 0\n",
      "time: 1.8146140575408936  count: 1000\n",
      "time: 1.7563879489898682  count: 2000\n",
      "time: 1.7607123851776123  count: 3000\n",
      "time: 1.7199733257293701  count: 4000\n",
      "time: 1.988532304763794  count: 5000\n",
      "time: 2.1288840770721436  count: 6000\n",
      "Creating .rec file from /tmp/caltech_objects_test.lst in /tmp\n",
      "time: 0.028704404830932617  count: 0\n",
      "time: 1.7193095684051514  count: 1000\n",
      "time: 1.6827032566070557  count: 2000\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  47M Nov 22 08:21 caltech_objects_test.rec\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 108M Nov 22 08:21 caltech_objects_train.rec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use the IM2REC script to convert our images into RecordIO files\n",
    "\n",
    "cd $BASE_DIR\n",
    "\n",
    "rm ${DATASET_NAME}_classes\n",
    "\n",
    "# First we need to create two LST files (training and test lists), noting the correct label class for each image\n",
    "# We'll also save the output of the LST files command, since it includes a list of all of our label classes\n",
    "echo \"Creating LST files\"\n",
    "python $IM2REC --list --recursive --pass-through --test-ratio=0.3 --train-ratio=0.7 $DATASET_NAME $DATASET_NAME > ${DATASET_NAME}_classes\n",
    "\n",
    "# Then we create RecordIO files from the LST files\n",
    "echo \"Creating RecordIO files\"\n",
    "rm *.rec\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_train.lst $DATASET_NAME\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_test.lst $DATASET_NAME\n",
    "ls -lh *.rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BACKGROUND_Google',\n",
       " 'Faces',\n",
       " 'Faces_easy',\n",
       " 'Leopards',\n",
       " 'Motorbikes',\n",
       " 'accordion',\n",
       " 'airplanes',\n",
       " 'anchor',\n",
       " 'ant',\n",
       " 'barrel']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of class labels into a python variable to use later\n",
    "classes_file_name = dataset_name + '_classes'\n",
    "path = base_dir + '/' + classes_file_name\n",
    "class_labels = ! cut -d \" \" -f1 {path} | paste -sd \" \"\n",
    "class_labels = class_labels[0].split(' ')\n",
    "\n",
    "# Show a few of them now just so we can see the kind of labels we're working with...\n",
    "class_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-west-2-541003905521/caltech_objects/train/caltech_objects_train.rec\n",
      "delete: s3://sagemaker-us-west-2-541003905521/caltech_objects/validation/caltech_objects_test.rec\n",
      "upload: ../../../tmp/caltech_objects_train.rec to s3://sagemaker-us-west-2-541003905521/caltech_objects/train/caltech_objects_train.rec\n",
      "upload: ../../../tmp/caltech_objects_test.rec to s3://sagemaker-us-west-2-541003905521/caltech_objects/validation/caltech_objects_test.rec\n"
     ]
    }
   ],
   "source": [
    "# Upload our train and test RecordIO files to S3 in the bucket that our sagemaker session is using\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "s3train_path = 's3://{}/{}/train/'.format(bucket, dataset_name)\n",
    "s3validation_path = 's3://{}/{}/validation/'.format(bucket, dataset_name)\n",
    "\n",
    "# Clean up any existing data in our training s3 bucket\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/train --recursive\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/validation --recursive\n",
    "\n",
    "# Upload the rec files to the train and validation channels\n",
    "!aws s3 cp /tmp/{dataset_name}_train.rec $s3train_path\n",
    "!aws s3 cp /tmp/{dataset_name}_test.rec $s3validation_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the data for our model training to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3train_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3validation_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, dataset_name)\n",
    "image_classifier = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count=2, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_wait=60*60,\n",
    "    train_max_run=60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_pretrained_model': 1,\n",
       " 'image_shape': '3,224,224',\n",
       " 'num_classes': 102,\n",
       " 'num_training_samples': 6400,\n",
       " 'epochs': 60,\n",
       " 'learning_rate': 0.001,\n",
       " 'mini_batch_size': 64,\n",
       " 'early_stopping': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes=! ls -l {base_dir}/{dataset_name} | wc -l\n",
    "num_classes=int(num_classes[0]) - 1\n",
    "\n",
    "num_training_samples=! cat {base_dir}/{dataset_name}_train.lst | wc -l\n",
    "num_training_samples = int(num_training_samples[0])\n",
    "\n",
    "# Learn more about the Sagemaker built-in Image Classifier hyperparameters here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "# These hyperparameters we won't want to change, as they define things like\n",
    "# the size of the images we'll be sending for input, the number of training classes we have, etc.\n",
    "base_hyperparameters=dict(\n",
    "    use_pretrained_model=1,\n",
    "    image_shape='3,224,224',\n",
    "    num_classes=num_classes,\n",
    "    num_training_samples=num_training_samples,\n",
    ")\n",
    "\n",
    "# These are hyperparameters we may want to tune, as they can affect the model training success:\n",
    "hyperparameters={\n",
    "    **base_hyperparameters, \n",
    "    **dict(\n",
    "        epochs=60,\n",
    "        learning_rate=0.001,\n",
    "        mini_batch_size=64,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "image_classifier.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "This will take some time because it's provisioning a new container runtime to train our model, then the actual training happens, then the trained model gets uploaded to S3 and the container is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-22 08:21:38 Starting - Starting the training job...\n",
      "2019-11-22 08:21:39 Starting - Launching requested ML instances......\n",
      "2019-11-22 08:22:42 Starting - Preparing the instances for training.........\n",
      "2019-11-22 08:24:27 Downloading - Downloading input data\n",
      "2019-11-22 08:24:27 Training - Downloading the training image...\n",
      "2019-11-22 08:24:57 Training - Training image download completed. Training in progress.\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:24:59 INFO 139634479433536] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:24:59 INFO 139634479433536] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'use_pretrained_model': u'1', u'epochs': u'60', u'num_training_samples': u'6400', u'num_classes': u'102', u'mini_batch_size': u'64', u'image_shape': u'3,224,224', u'early_stopping': u'True'}\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:24:59 INFO 139634479433536] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.001', u'epochs': u'60', u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'precision_dtype': u'float32', u'mini_batch_size': u'64', u'early_stopping': u'True', u'num_classes': u'102', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'6400'}\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/7580dc40-9121-45e1-997d-8c333c3bf79b', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-94-141.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dd52fdeb-df0b-4616-b717-996a7750175b', 'ALGO_INPUT_DIR': '/opt/ml/input', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/7580dc40-9121-45e1-997d-8c333c3bf79b', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.109.78', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-94-141.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dd52fdeb-df0b-4616-b717-996a7750175b', 'DMLC_ROLE': 'server', 'ALGO_INPUT_DIR': '/opt/ml/input', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/7580dc40-9121-45e1-997d-8c333c3bf79b', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.109.78', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-94-141.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/dd52fdeb-df0b-4616-b717-996a7750175b', 'DMLC_ROLE': 'worker', 'ALGO_INPUT_DIR': '/opt/ml/input', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] multi_label: 0\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] ---- Parameters ----\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] num_layers: 152\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] epochs: 60\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] optimizer: sgd\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] momentum: 0.9\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] learning_rate: 0.001\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] num_training_samples: 6400\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] mini_batch_size: 64\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] num_classes: 102\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] augmentation_type: None\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] kv_store: dist_sync\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Using early stopping for training\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Early stopping minimum epochs: 10\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Early stopping patience: 10\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] Early stopping tolerance: 0.01\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:00 INFO 139634479433536] --------------------\u001b[0m\n",
      "\u001b[32m[08:25:00] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[32m[08:25:00] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'use_pretrained_model': u'1', u'epochs': u'60', u'num_training_samples': u'6400', u'num_classes': u'102', u'mini_batch_size': u'64', u'image_shape': u'3,224,224', u'early_stopping': u'True'}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.001', u'epochs': u'60', u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'precision_dtype': u'float32', u'mini_batch_size': u'64', u'early_stopping': u'True', u'num_classes': u'102', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'6400'}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/280546af-510a-4f64-bd82-7f9c80aa44ad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-109-78.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/daf4ce7b-9e47-4b39-bb7e-283a49631e9f', 'ALGO_INPUT_DIR': '/opt/ml/input', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/280546af-510a-4f64-bd82-7f9c80aa44ad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.109.78', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-109-78.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/daf4ce7b-9e47-4b39-bb7e-283a49631e9f', 'DMLC_ROLE': 'scheduler', 'ALGO_INPUT_DIR': '/opt/ml/input', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/280546af-510a-4f64-bd82-7f9c80aa44ad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-109-78.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/daf4ce7b-9e47-4b39-bb7e-283a49631e9f', 'ALGO_INPUT_DIR': '/opt/ml/input', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/280546af-510a-4f64-bd82-7f9c80aa44ad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.109.78', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-109-78.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/daf4ce7b-9e47-4b39-bb7e-283a49631e9f', 'DMLC_ROLE': 'server', 'ALGO_INPUT_DIR': '/opt/ml/input', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/280546af-510a-4f64-bd82-7f9c80aa44ad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'ALGO_OUTPUT_DIR': '/opt/ml/output', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'ALGO_MODEL_DIR': '/opt/ml/model', 'IMAGE_CLASSIFICATION_PRETRAINED': '/pretrained', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.109.78', 'AWS_REGION': 'us-west-2', 'PWD': '/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'image-classification-2019-11-22-08-21-37-795', 'KMP_AFFINITY': 'granularity=fine,compact,1,0', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'ALGO_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'NVIDIA_DRIVER_CAPABILITIES': 'utility,compute', 'ALGO_INPUT_RESOURCE_CONFIG_FILE': '/opt/ml/input/config/resourceconfig.json', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-109-78.us-west-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/daf4ce7b-9e47-4b39-bb7e-283a49631e9f', 'DMLC_ROLE': 'worker', 'ALGO_INPUT_DIR': '/opt/ml/input', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:541003905521:training-job/image-classification-2019-11-22-08-21-37-795', 'ALGO_INPUT_DATA_DIR': '/opt/ml/input/data', 'ALGO_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json'}\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] multi_label: 0\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] ---- Parameters ----\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] num_layers: 152\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] epochs: 60\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] optimizer: sgd\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] momentum: 0.9\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] learning_rate: 0.001\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] num_training_samples: 6400\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] mini_batch_size: 64\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] num_classes: 102\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] augmentation_type: None\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] kv_store: dist_sync\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Using early stopping for training\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Early stopping minimum epochs: 10\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Early stopping patience: 10\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] Early stopping tolerance: 0.01\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:01 INFO 140186290300736] --------------------\u001b[0m\n",
      "\u001b[31m[08:25:01] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[31m[08:25:01] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:02 INFO 139634479433536] Setting number of threads: 7\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:03 INFO 140186290300736] Setting number of threads: 7\u001b[0m\n",
      "\u001b[31m[08:25:12] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[32m[08:25:12] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:27 INFO 139634479433536] Epoch[0] Batch [20]#011Speed: 82.149 samples/sec#011accuracy=0.154762\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:27 INFO 140186290300736] Epoch[0] Batch [20]#011Speed: 82.182 samples/sec#011accuracy=0.154018\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:40 INFO 139634479433536] Epoch[0] Batch [40]#011Speed: 89.954 samples/sec#011accuracy=0.249238\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:40 INFO 140186290300736] Epoch[0] Batch [40]#011Speed: 89.937 samples/sec#011accuracy=0.243140\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:46 INFO 139634479433536] Epoch[0] Train-accuracy=0.273438\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:46 INFO 139634479433536] Epoch[0] Time cost=33.989\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:46 INFO 140186290300736] Epoch[0] Train-accuracy=0.275625\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:46 INFO 140186290300736] Epoch[0] Time cost=33.979\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:50 INFO 139634479433536] Epoch[0] Validation-accuracy=0.395597\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:25:50 INFO 140186290300736] Epoch[0] Validation-accuracy=0.425426\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:50 INFO 139634479433536] Storing the best model with validation accuracy: 0.395597\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:25:50 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:04 INFO 140186290300736] Epoch[1] Batch [20]#011Speed: 93.114 samples/sec#011accuracy=0.476935\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:04 INFO 139634479433536] Epoch[1] Batch [20]#011Speed: 93.029 samples/sec#011accuracy=0.470238\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:17 INFO 140186290300736] Epoch[1] Batch [40]#011Speed: 96.685 samples/sec#011accuracy=0.518674\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:17 INFO 139634479433536] Epoch[1] Batch [40]#011Speed: 96.668 samples/sec#011accuracy=0.517530\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:23 INFO 139634479433536] Epoch[1] Train-accuracy=0.536250\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:23 INFO 139634479433536] Epoch[1] Time cost=32.110\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:23 INFO 140186290300736] Epoch[1] Train-accuracy=0.533125\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:23 INFO 140186290300736] Epoch[1] Time cost=32.096\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:26 INFO 139634479433536] Epoch[1] Validation-accuracy=0.610863\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:27 INFO 139634479433536] Storing the best model with validation accuracy: 0.610863\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:26 INFO 140186290300736] Epoch[1] Validation-accuracy=0.624256\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:27 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:41 INFO 139634479433536] Epoch[2] Batch [20]#011Speed: 96.083 samples/sec#011accuracy=0.675595\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:41 INFO 140186290300736] Epoch[2] Batch [20]#011Speed: 96.178 samples/sec#011accuracy=0.671875\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:53 INFO 140186290300736] Epoch[2] Batch [40]#011Speed: 99.280 samples/sec#011accuracy=0.704649\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:53 INFO 139634479433536] Epoch[2] Batch [40]#011Speed: 99.332 samples/sec#011accuracy=0.713796\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:58 INFO 139634479433536] Epoch[2] Train-accuracy=0.727187\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:26:58 INFO 139634479433536] Epoch[2] Time cost=31.268\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:58 INFO 140186290300736] Epoch[2] Train-accuracy=0.720000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:26:58 INFO 140186290300736] Epoch[2] Time cost=31.270\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:02 INFO 139634479433536] Epoch[2] Validation-accuracy=0.760653\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:03 INFO 139634479433536] Storing the best model with validation accuracy: 0.760653\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:27:02 INFO 140186290300736] Epoch[2] Validation-accuracy=0.764915\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:03 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:27:16 INFO 140186290300736] Epoch[3] Batch [20]#011Speed: 98.108 samples/sec#011accuracy=0.830357\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:16 INFO 139634479433536] Epoch[3] Batch [20]#011Speed: 97.950 samples/sec#011accuracy=0.816220\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:29 INFO 139634479433536] Epoch[3] Batch [40]#011Speed: 100.547 samples/sec#011accuracy=0.841463\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:27:29 INFO 140186290300736] Epoch[3] Batch [40]#011Speed: 100.511 samples/sec#011accuracy=0.849848\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:27:34 INFO 140186290300736] Epoch[3] Train-accuracy=0.856875\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:27:34 INFO 140186290300736] Epoch[3] Time cost=30.987\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:34 INFO 139634479433536] Epoch[3] Train-accuracy=0.847812\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:34 INFO 139634479433536] Epoch[3] Time cost=30.971\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:38 INFO 139634479433536] Epoch[3] Validation-accuracy=0.842262\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:27:38 INFO 140186290300736] Epoch[3] Validation-accuracy=0.843750\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:38 INFO 139634479433536] Storing the best model with validation accuracy: 0.842262\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:39 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0004.params\"\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:27:52 INFO 139634479433536] Epoch[4] Batch [20]#011Speed: 96.818 samples/sec#011accuracy=0.912202\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:27:52 INFO 140186290300736] Epoch[4] Batch [20]#011Speed: 96.815 samples/sec#011accuracy=0.912202\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:05 INFO 139634479433536] Epoch[4] Batch [40]#011Speed: 98.688 samples/sec#011accuracy=0.921875\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:05 INFO 140186290300736] Epoch[4] Batch [40]#011Speed: 98.688 samples/sec#011accuracy=0.921875\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:10 INFO 140186290300736] Epoch[4] Train-accuracy=0.929063\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:10 INFO 140186290300736] Epoch[4] Time cost=31.488\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:10 INFO 139634479433536] Epoch[4] Train-accuracy=0.925937\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:10 INFO 139634479433536] Epoch[4] Time cost=31.488\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:14 INFO 140186290300736] Epoch[4] Validation-accuracy=0.895089\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:14 INFO 139634479433536] Epoch[4] Validation-accuracy=0.894176\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:14 INFO 139634479433536] Storing the best model with validation accuracy: 0.894176\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:15 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:28 INFO 140186290300736] Epoch[5] Batch [20]#011Speed: 94.911 samples/sec#011accuracy=0.952381\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:28 INFO 139634479433536] Epoch[5] Batch [20]#011Speed: 94.602 samples/sec#011accuracy=0.954613\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:41 INFO 139634479433536] Epoch[5] Batch [40]#011Speed: 97.832 samples/sec#011accuracy=0.959985\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:41 INFO 140186290300736] Epoch[5] Batch [40]#011Speed: 97.818 samples/sec#011accuracy=0.955412\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:47 INFO 139634479433536] Epoch[5] Train-accuracy=0.961250\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:47 INFO 139634479433536] Epoch[5] Time cost=31.897\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:47 INFO 140186290300736] Epoch[5] Train-accuracy=0.960000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:47 INFO 140186290300736] Epoch[5] Time cost=31.929\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:50 INFO 139634479433536] Epoch[5] Validation-accuracy=0.918155\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:51 INFO 139634479433536] Storing the best model with validation accuracy: 0.918155\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:28:51 INFO 140186290300736] Epoch[5] Validation-accuracy=0.920455\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:28:51 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0006.params\"\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:05 INFO 139634479433536] Epoch[6] Batch [20]#011Speed: 94.905 samples/sec#011accuracy=0.975446\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:05 INFO 140186290300736] Epoch[6] Batch [20]#011Speed: 94.925 samples/sec#011accuracy=0.973214\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:17 INFO 139634479433536] Epoch[6] Batch [40]#011Speed: 98.585 samples/sec#011accuracy=0.976372\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:17 INFO 140186290300736] Epoch[6] Batch [40]#011Speed: 98.590 samples/sec#011accuracy=0.977896\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:23 INFO 140186290300736] Epoch[6] Train-accuracy=0.979375\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:23 INFO 140186290300736] Epoch[6] Time cost=31.381\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:23 INFO 139634479433536] Epoch[6] Train-accuracy=0.978437\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:23 INFO 139634479433536] Epoch[6] Time cost=31.423\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:26 INFO 140186290300736] Epoch[6] Validation-accuracy=0.936012\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:26 INFO 139634479433536] Epoch[6] Validation-accuracy=0.934659\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:27 INFO 139634479433536] Storing the best model with validation accuracy: 0.934659\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:27 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0007.params\"\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:40 INFO 140186290300736] Epoch[7] Batch [20]#011Speed: 96.177 samples/sec#011accuracy=0.979911\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:40 INFO 139634479433536] Epoch[7] Batch [20]#011Speed: 96.190 samples/sec#011accuracy=0.984375\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:53 INFO 139634479433536] Epoch[7] Batch [40]#011Speed: 99.304 samples/sec#011accuracy=0.983994\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:53 INFO 140186290300736] Epoch[7] Batch [40]#011Speed: 99.352 samples/sec#011accuracy=0.985137\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:58 INFO 140186290300736] Epoch[7] Train-accuracy=0.986563\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:29:58 INFO 140186290300736] Epoch[7] Time cost=31.214\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:58 INFO 139634479433536] Epoch[7] Train-accuracy=0.985625\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:29:58 INFO 139634479433536] Epoch[7] Time cost=31.208\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:02 INFO 139634479433536] Epoch[7] Validation-accuracy=0.941220\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:30:02 INFO 140186290300736] Epoch[7] Validation-accuracy=0.940341\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:02 INFO 139634479433536] Storing the best model with validation accuracy: 0.941220\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:03 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0008.params\"\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:16 INFO 139634479433536] Epoch[8] Batch [20]#011Speed: 97.152 samples/sec#011accuracy=0.991815\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:30:16 INFO 140186290300736] Epoch[8] Batch [20]#011Speed: 97.109 samples/sec#011accuracy=0.988095\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:30:29 INFO 140186290300736] Epoch[8] Batch [40]#011Speed: 99.766 samples/sec#011accuracy=0.990091\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:29 INFO 139634479433536] Epoch[8] Batch [40]#011Speed: 99.779 samples/sec#011accuracy=0.990091\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:34 INFO 139634479433536] Epoch[8] Train-accuracy=0.990313\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:34 INFO 139634479433536] Epoch[8] Time cost=31.160\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:30:34 INFO 140186290300736] Epoch[8] Train-accuracy=0.991250\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:30:34 INFO 140186290300736] Epoch[8] Time cost=31.162\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:38 INFO 139634479433536] Epoch[8] Validation-accuracy=0.942472\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:30:38 INFO 140186290300736] Epoch[8] Validation-accuracy=0.942708\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:38 INFO 139634479433536] Storing the best model with validation accuracy: 0.942472\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:39 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0009.params\"\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:30:52 INFO 139634479433536] Epoch[9] Batch [20]#011Speed: 97.427 samples/sec#011accuracy=0.996280\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:30:52 INFO 140186290300736] Epoch[9] Batch [20]#011Speed: 97.253 samples/sec#011accuracy=0.991815\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:05 INFO 140186290300736] Epoch[9] Batch [40]#011Speed: 99.330 samples/sec#011accuracy=0.993140\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:05 INFO 139634479433536] Epoch[9] Batch [40]#011Speed: 99.389 samples/sec#011accuracy=0.995046\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:10 INFO 139634479433536] Epoch[9] Train-accuracy=0.995625\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:10 INFO 139634479433536] Epoch[9] Time cost=31.238\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:10 INFO 140186290300736] Epoch[9] Train-accuracy=0.993750\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:10 INFO 140186290300736] Epoch[9] Time cost=31.238\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:14 INFO 139634479433536] Epoch[9] Validation-accuracy=0.941964\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:14 INFO 140186290300736] Epoch[9] Validation-accuracy=0.946429\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:28 INFO 140186290300736] Epoch[10] Batch [20]#011Speed: 96.181 samples/sec#011accuracy=0.996280\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:28 INFO 139634479433536] Epoch[10] Batch [20]#011Speed: 96.253 samples/sec#011accuracy=0.996280\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:40 INFO 139634479433536] Epoch[10] Batch [40]#011Speed: 98.134 samples/sec#011accuracy=0.996570\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:40 INFO 140186290300736] Epoch[10] Batch [40]#011Speed: 98.137 samples/sec#011accuracy=0.996189\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:46 INFO 139634479433536] Epoch[10] Train-accuracy=0.996250\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:46 INFO 139634479433536] Epoch[10] Time cost=31.610\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:46 INFO 140186290300736] Epoch[10] Train-accuracy=0.996563\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:46 INFO 140186290300736] Epoch[10] Time cost=31.613\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:31:50 INFO 140186290300736] Epoch[10] Validation-accuracy=0.948153\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:31:50 INFO 139634479433536] Epoch[10] Validation-accuracy=0.942472\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:04 INFO 140186290300736] Epoch[11] Batch [20]#011Speed: 96.284 samples/sec#011accuracy=0.997024\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:04 INFO 139634479433536] Epoch[11] Batch [20]#011Speed: 95.958 samples/sec#011accuracy=0.998512\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:16 INFO 139634479433536] Epoch[11] Batch [40]#011Speed: 98.358 samples/sec#011accuracy=0.997332\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:16 INFO 140186290300736] Epoch[11] Batch [40]#011Speed: 98.363 samples/sec#011accuracy=0.998095\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:22 INFO 139634479433536] Epoch[11] Train-accuracy=0.997500\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:22 INFO 139634479433536] Epoch[11] Time cost=31.908\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:22 INFO 140186290300736] Epoch[11] Train-accuracy=0.998437\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:22 INFO 140186290300736] Epoch[11] Time cost=31.939\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:26 INFO 139634479433536] Epoch[11] Validation-accuracy=0.944940\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:26 INFO 139634479433536] Storing the best model with validation accuracy: 0.944940\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:26 INFO 140186290300736] Epoch[11] Validation-accuracy=0.947173\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:27 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0012.params\"\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:40 INFO 139634479433536] Epoch[12] Batch [20]#011Speed: 94.657 samples/sec#011accuracy=0.997768\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:40 INFO 140186290300736] Epoch[12] Batch [20]#011Speed: 94.404 samples/sec#011accuracy=0.998512\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:53 INFO 139634479433536] Epoch[12] Batch [40]#011Speed: 96.692 samples/sec#011accuracy=0.997332\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:53 INFO 140186290300736] Epoch[12] Batch [40]#011Speed: 96.614 samples/sec#011accuracy=0.998095\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:59 INFO 140186290300736] Epoch[12] Train-accuracy=0.998437\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:32:59 INFO 140186290300736] Epoch[12] Time cost=32.042\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:59 INFO 139634479433536] Epoch[12] Train-accuracy=0.997812\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:32:59 INFO 139634479433536] Epoch[12] Time cost=32.039\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:33:03 INFO 139634479433536] Epoch[12] Validation-accuracy=0.944602\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:33:03 INFO 140186290300736] Epoch[12] Validation-accuracy=0.947443\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:33:16 INFO 139634479433536] Epoch[13] Batch [20]#011Speed: 96.116 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:33:16 INFO 140186290300736] Epoch[13] Batch [20]#011Speed: 96.174 samples/sec#011accuracy=0.997768\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:33:29 INFO 140186290300736] Epoch[13] Batch [40]#011Speed: 99.152 samples/sec#011accuracy=0.998857\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:33:29 INFO 139634479433536] Epoch[13] Batch [40]#011Speed: 99.190 samples/sec#011accuracy=0.999238\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:33:34 INFO 139634479433536] Epoch[13] Train-accuracy=0.999375\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:33:34 INFO 139634479433536] Epoch[13] Time cost=31.215\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:33:34 INFO 140186290300736] Epoch[13] Train-accuracy=0.999062\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:33:34 INFO 140186290300736] Epoch[13] Time cost=31.225\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:33:38 INFO 139634479433536] Epoch[13] Validation-accuracy=0.944940\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:33:38 INFO 140186290300736] Epoch[13] Validation-accuracy=0.947173\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:33:52 INFO 139634479433536] Epoch[14] Batch [20]#011Speed: 97.323 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:33:52 INFO 140186290300736] Epoch[14] Batch [20]#011Speed: 97.344 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:04 INFO 139634479433536] Epoch[14] Batch [40]#011Speed: 99.643 samples/sec#011accuracy=0.999619\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:04 INFO 140186290300736] Epoch[14] Batch [40]#011Speed: 99.590 samples/sec#011accuracy=0.999619\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:10 INFO 139634479433536] Epoch[14] Train-accuracy=0.999687\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:10 INFO 139634479433536] Epoch[14] Time cost=31.135\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:10 INFO 140186290300736] Epoch[14] Train-accuracy=0.999687\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:10 INFO 140186290300736] Epoch[14] Time cost=31.138\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:13 INFO 139634479433536] Epoch[14] Validation-accuracy=0.946733\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:14 INFO 139634479433536] Storing the best model with validation accuracy: 0.946733\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:13 INFO 140186290300736] Epoch[14] Validation-accuracy=0.948661\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:14 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0015.params\"\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:27 INFO 139634479433536] Epoch[15] Batch [20]#011Speed: 98.480 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:27 INFO 140186290300736] Epoch[15] Batch [20]#011Speed: 98.483 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:40 INFO 139634479433536] Epoch[15] Batch [40]#011Speed: 100.582 samples/sec#011accuracy=0.999619\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:40 INFO 140186290300736] Epoch[15] Batch [40]#011Speed: 100.577 samples/sec#011accuracy=0.999619\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:45 INFO 140186290300736] Epoch[15] Train-accuracy=0.999375\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:45 INFO 140186290300736] Epoch[15] Time cost=30.875\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:45 INFO 139634479433536] Epoch[15] Train-accuracy=0.999375\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:45 INFO 139634479433536] Epoch[15] Time cost=30.868\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:34:49 INFO 139634479433536] Epoch[15] Validation-accuracy=0.944940\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:34:49 INFO 140186290300736] Epoch[15] Validation-accuracy=0.946733\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:03 INFO 139634479433536] Epoch[16] Batch [20]#011Speed: 94.837 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:03 INFO 140186290300736] Epoch[16] Batch [20]#011Speed: 94.590 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:16 INFO 139634479433536] Epoch[16] Batch [40]#011Speed: 96.751 samples/sec#011accuracy=0.999238\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:16 INFO 140186290300736] Epoch[16] Batch [40]#011Speed: 96.743 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:21 INFO 139634479433536] Epoch[16] Train-accuracy=0.999375\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:21 INFO 139634479433536] Epoch[16] Time cost=31.926\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:21 INFO 140186290300736] Epoch[16] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:21 INFO 140186290300736] Epoch[16] Time cost=31.927\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:25 INFO 140186290300736] Epoch[16] Validation-accuracy=0.947917\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:25 INFO 139634479433536] Epoch[16] Validation-accuracy=0.945685\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:39 INFO 139634479433536] Epoch[17] Batch [20]#011Speed: 96.369 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:39 INFO 140186290300736] Epoch[17] Batch [20]#011Speed: 96.712 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:51 INFO 139634479433536] Epoch[17] Batch [40]#011Speed: 99.405 samples/sec#011accuracy=0.999619\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:51 INFO 140186290300736] Epoch[17] Batch [40]#011Speed: 99.405 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:57 INFO 139634479433536] Epoch[17] Train-accuracy=0.999687\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:35:57 INFO 139634479433536] Epoch[17] Time cost=31.502\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:57 INFO 140186290300736] Epoch[17] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:35:57 INFO 140186290300736] Epoch[17] Time cost=31.500\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:36:01 INFO 139634479433536] Epoch[17] Validation-accuracy=0.946733\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:36:01 INFO 140186290300736] Epoch[17] Validation-accuracy=0.948864\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:36:15 INFO 139634479433536] Epoch[18] Batch [20]#011Speed: 95.107 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:36:15 INFO 140186290300736] Epoch[18] Batch [20]#011Speed: 94.859 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:36:28 INFO 139634479433536] Epoch[18] Batch [40]#011Speed: 97.398 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:36:28 INFO 140186290300736] Epoch[18] Batch [40]#011Speed: 97.384 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:36:33 INFO 139634479433536] Epoch[18] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:36:33 INFO 139634479433536] Epoch[18] Time cost=31.763\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:36:33 INFO 140186290300736] Epoch[18] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:36:33 INFO 140186290300736] Epoch[18] Time cost=31.742\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:36:37 INFO 139634479433536] Epoch[18] Validation-accuracy=0.945685\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:36:37 INFO 140186290300736] Epoch[18] Validation-accuracy=0.947917\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:36:51 INFO 139634479433536] Epoch[19] Batch [20]#011Speed: 96.168 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:36:51 INFO 140186290300736] Epoch[19] Batch [20]#011Speed: 96.176 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:03 INFO 140186290300736] Epoch[19] Batch [40]#011Speed: 98.624 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:03 INFO 139634479433536] Epoch[19] Batch [40]#011Speed: 98.672 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:09 INFO 139634479433536] Epoch[19] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:09 INFO 139634479433536] Epoch[19] Time cost=31.444\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:09 INFO 140186290300736] Epoch[19] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:09 INFO 140186290300736] Epoch[19] Time cost=31.455\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:13 INFO 139634479433536] Epoch[19] Validation-accuracy=0.946733\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:12 INFO 140186290300736] Epoch[19] Validation-accuracy=0.946429\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:26 INFO 139634479433536] Epoch[20] Batch [20]#011Speed: 96.109 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:26 INFO 140186290300736] Epoch[20] Batch [20]#011Speed: 96.427 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:39 INFO 140186290300736] Epoch[20] Batch [40]#011Speed: 97.781 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:39 INFO 139634479433536] Epoch[20] Batch [40]#011Speed: 97.808 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:45 INFO 140186290300736] Epoch[20] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:45 INFO 140186290300736] Epoch[20] Time cost=31.684\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:49 INFO 140186290300736] Epoch[20] Validation-accuracy=0.950284\u001b[0m\n",
      "\u001b[31m[11/22/2019 08:37:49 INFO 140186290300736] Early stopping criteria met...\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:45 INFO 139634479433536] Epoch[20] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:45 INFO 139634479433536] Epoch[20] Time cost=31.677\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:48 INFO 139634479433536] Epoch[20] Validation-accuracy=0.947173\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:49 INFO 139634479433536] Early stopping criteria met.\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:49 INFO 139634479433536] Storing the best model with validation accuracy: 0.947173\u001b[0m\n",
      "\u001b[32m[11/22/2019 08:37:49 INFO 139634479433536] Saved checkpoint to \"/opt/ml/model/image-classification-0021.params\"\u001b[0m\n",
      "\n",
      "2019-11-22 08:37:53 Uploading - Uploading generated training model\n",
      "2019-11-22 08:38:26 Completed - Training job completed\n",
      "Training seconds: 1706\n",
      "Billable seconds: 512\n",
      "Managed Spot Training savings: 70.0%\n",
      "\n",
      "\n",
      " Finished training! The model is available for download at: s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-22-08-21-37-795/output/model.tar.gz\n",
      "CPU times: user 2.66 s, sys: 117 ms, total: 2.77 s\n",
      "Wall time: 17min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "image_classifier.fit(inputs=data_channels, logs=True)\n",
    "\n",
    "job = image_classifier.latest_training_job\n",
    "model_path = f\"{base_dir}/{job.name}\"\n",
    "\n",
    "print(f\"\\n\\n Finished training! The model is available for download at: {image_classifier.output_path}/{job.name}/output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting our SageMaker-trained model to the ONNX Format\n",
    "\n",
    "SageMaker uses a framework called MXNet to train and produce our image classifier model.  But, we might want to use this model to perform inference somehwere that MXNet doesn't easily run, such as in a web browser.  \n",
    "\n",
    "ONNX is an open format to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools and choose the combination that is best for them. More info at https://onnx.ai/\n",
    "\n",
    "SageMaker provides helpful tooling in the Python SDK to convert trained models to the ONNX format, making it easy to take your trained model, convert it to ONNX, then use that model in whatever environment you want to (as long as that environment will accept models in the ONNX format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_S3_PATH=s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-22-08-21-37-795/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Get the path where our trained model was saved to S3\n",
    "model_s3_path = image_classifier.model_data\n",
    "model_s3_output_dir = \"/\".join(model_s3_path.split('/')[0:-1])\n",
    "%env MODEL_S3_PATH = $model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-22-08-21-37-795/output/model.tar.gz to ../../../tmp/downloaded_model/model.tar.gz\n",
      "image-classification-0021.params\n",
      "image-classification-symbol.json\n",
      "model-shapes.json\n",
      "model-shapes.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download our model and extract it into $BASE_DIR/downloaded_model\n",
    "rm -rf $BASE_DIR/downloaded_model/\n",
    "mkdir -p $BASE_DIR/downloaded_model/\n",
    "aws s3 cp $MODEL_S3_PATH $BASE_DIR/downloaded_model/model.tar.gz\n",
    "cd $BASE_DIR/downloaded_model\n",
    "tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../tmp/downloaded_model/model.onnx to s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-22-08-21-37-795/output/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Use MXNet's onnx_mxnet module to convert the MXNet model that SageMaker trained into ONNX format\n",
    "from mxnet.contrib import onnx as onnx_mxnet\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "model_dir = base_dir + \"/downloaded_model\"\n",
    "\n",
    "model_symbol_file = glob(model_dir + \"/*symbol.json\")[0]\n",
    "model_params_file = glob(model_dir + \"/*.params\")[0]\n",
    "\n",
    "onnx_mxnet.export_model(sym=model_symbol_file,\n",
    "                            params=model_params_file,\n",
    "                            input_shape=[(1, 3, 224, 224)],\n",
    "                            input_type=np.float32,\n",
    "                            onnx_file_path=\"{}/model.onnx\".format(model_dir),\n",
    "                            verbose=True)\n",
    "\n",
    "# And upload the ONNX model back to the same place on S3 as where we Sagemaker put the MXNet version of the model, just for safe-keeping\n",
    "! aws s3 cp {model_dir}/model.onnx {model_s3_output_dir}/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Click here to download the model in ONNX format: <a href='model.onnx' download=model.onnx>model.onnx</a><br>"
      ],
      "text/plain": [
       "/home/ec2-user/SageMaker/model.onnx"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, display a link so we can download the ONNX version of the model easily from this notebook's local disk\n",
    "\n",
    "from IPython.display import FileLink\n",
    "# DownloadFileLink via https://github.com/jupyterlab/jupyterlab/issues/5443\n",
    "class DownloadFileLink(FileLink):\n",
    "    html_link_str = \"<a href='{link}' download={file_name}>{link_text}</a>\"\n",
    "\n",
    "    def __init__(self, path, file_name=None, link_text=None, *args, **kwargs):\n",
    "        super(DownloadFileLink, self).__init__(path, *args, **kwargs)\n",
    "\n",
    "        self.file_name = file_name or os.path.split(path)[1]\n",
    "        self.link_text = link_text or self.file_name\n",
    "\n",
    "    def _format_path(self):\n",
    "        from html import escape\n",
    "        fp = ''.join([self.url_prefix, escape(self.path)])\n",
    "        return ''.join([self.result_html_prefix,\n",
    "                        self.html_link_str.format(link=fp, file_name=self.file_name, link_text=self.link_text),\n",
    "                        self.result_html_suffix])\n",
    "    \n",
    "# We'll need to symlink the onnx model file from base_dir/downloaded_model/model.onnx to this notebook's home directory \n",
    "# so that Jupyter will serve it\n",
    "! ln -fs {model_dir}/model.onnx ./model.onnx\n",
    "\n",
    "# Output the download link for us to click on\n",
    "DownloadFileLink(\"model.onnx\", result_html_prefix=\"Click here to download the model in ONNX format: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut: {base_dir}/{classes_file_name}: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Finally, we'll also want the handy list of knowing all the classes that our model returns scores for. \n",
    "# Our model returns a score for each label, in this same order.\n",
    "\n",
    "print(' '.join(class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cut:',\n",
       " '{base_dir}/{classes_file_name}:',\n",
       " 'No',\n",
       " 'such',\n",
       " 'file',\n",
       " 'or',\n",
       " 'directory']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ONNX Model for inference\n",
    "\n",
    "Now that you have an ONNX format of your image classifer model downloaded, \n",
    "you can use that model, along with the list of class labels (shown above),\n",
    "to make inferences about images any place that can run ONNX models, including \n",
    "offline, in your web browser! \n",
    "\n",
    "Want to give that a try? Download your ONNX model, copy the class labels list above,\n",
    "then visit this URL which will load your model into your web browser (it doesn't upload the model anywhere)\n",
    "and make inferences on images that you drag and drop onto the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Using SageMaker to host a deployed version of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------!CPU times: user 686 ms, sys: 29 ms, total: 715 ms\n",
      "Wall time: 11min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Deploying a model to an endpoint takes a few minutes to complete\n",
    "\n",
    "# assert False # Change this to True to deploy. This takes about 10 minutes to complete this step.\n",
    "\n",
    "import time\n",
    "now = str(int(time.time()))\n",
    "\n",
    "deployed_endpoint = image_classifier.deploy(\n",
    "    endpoint_name = dataset_name.replace('_', '-') + '-' + now,\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.t2.medium',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling a deployed endpoint from Python code\n",
    "\n",
    "Once you've deployed to a SakeMaker hosted endpoint, you'll want to pass it some images to see it perform inferences.  Here's how to do this from some python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def classify_deployed(file_name, classes):\n",
    "    payload = None\n",
    "    with open(file_name, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    deployed_endpoint.content_type = 'application/x-image'\n",
    "    result = json.loads(deployed_endpoint.predict(payload))\n",
    "    print(result)\n",
    "    best_prob_index = np.argmax(result)\n",
    "    return (classes[best_prob_index], result[best_prob_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_url=\"https://some/image/to/download\"\n",
    "\n",
    "# Download an image from a URL\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# And resize it\n",
    "size = 224\n",
    "new_im = Image.new('RGB', (size, size), (0, 0, 0, 0))\n",
    "img.thumbnail([size, size])\n",
    "(w, h) = img.size\n",
    "new_im.paste(img, (int((size - w) / 2), int((size - h) / 2 )))\n",
    "new_im.save(tmp_local_path, \"JPEG\", quality = 95)\n",
    "\n",
    "# Show it\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=tmp_local_path))\n",
    "\n",
    "# Classify it\n",
    "classify_deployed(tmp_local_path, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Perform Hyperparameter Tuning\n",
    "\n",
    "Often, you might not know which values for hyperparameters like `learning_rate` and `mini_batch_size` will yield acceptible results. Traditionally, this meant manually running many training jobs with different hyperparameter values, looking at each trained model's performance, and then picking a winner. \n",
    "\n",
    "\n",
    "This type of manual tuning is _very_ time consuming, so you can automate this process using automatic model tuning with SageMaker. Here's some example code to illustrate how to start one of these jobs using the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ---------------\n",
    "\n",
    "assert False # Change to True to run this\n",
    "\n",
    "# ---------------\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "hyperparameter_ranges = {\n",
    "    'optimizer': CategoricalParameter(['sgd', 'adam', 'rmsprop', 'nag']),\n",
    "     'learning_rate': ContinuousParameter(0.0001, 0.1),\n",
    "     'mini_batch_size': CategoricalParameter([4, 8, 16, 32]),\n",
    "     'momentum': ContinuousParameter(0.0, 0.99),                                                                   \n",
    "     'weight_decay': ContinuousParameter(0.0, 0.99),   \n",
    "}\n",
    "\n",
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    image_classifier,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    early_stopping_type='Off',\n",
    "    max_jobs=500,\n",
    "    max_parallel_jobs=2\n",
    ")\n",
    "\n",
    "\n",
    "tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "When you're done learning here, you'll likley want to delete the endpoint you deployed (if you did so), since that endpoint will charge you money even if you're not actively making inferences (because you're charged for the total availability time of the endpoint). Here's how to clean up the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "deployed_endpoint.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
