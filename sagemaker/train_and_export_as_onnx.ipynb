{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some variables for the local directories we'll work in for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=/tmp\n",
      "env: DATASET_DIR=/tmp/caltech_objects\n",
      "env: DATASET_NAME=caltech_objects\n"
     ]
    }
   ],
   "source": [
    "base_dir='/tmp'\n",
    "dataset_name = 'caltech_objects'\n",
    "dataset_dir = base_dir + '/' + dataset_name\n",
    "\n",
    "%env BASE_DIR=$base_dir\n",
    "%env DATASET_DIR=$dataset_dir\n",
    "%env DATASET_NAME=$dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab a bunch of images grouped by folders, one per label class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf $DATASET_DIR\n",
    "mkdir -p $DATASET_DIR\n",
    "cd $DATASET_DIR\n",
    "wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz --quiet\n",
    "tar -xzf 101_ObjectCategories.tar.gz\n",
    "mv 101_ObjectCategories/* .\n",
    "rm -rf 101_ObjectCategories\n",
    "rm 101_ObjectCategories.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some variables for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: IM2REC=/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/tools/im2rec.py\n"
     ]
    }
   ],
   "source": [
    "# Find im2rec in our environment and set up some other vars in our environemnt\n",
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter( (lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "%env IM2REC=$im2rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to 224x224\n",
    "# Sometimes, our input images aren't already in the desired format for training. I like to format all my images to be the correct size before I train my model.\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def resize(filepath, size=224, fill_color=(0, 0, 0, 0)):\n",
    "    try:\n",
    "        image = Image.open(filepath)\n",
    "    except:\n",
    "        os.remove(filepath)\n",
    "        return\n",
    "\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    image.thumbnail([size, size])\n",
    "    (w, h) = image.size\n",
    "    new_im.paste(image, (int((size - w) / 2), int((size - h) / 2 )))\n",
    "\n",
    "    # Overwrite file with resized version\n",
    "    new_im.save(filepath, \"JPEG\", quality = 95)\n",
    "\n",
    "def recursively_process_files(dirname, processing_func):\n",
    "    for dirname, dirnames, filenames in os.walk(dirname):\n",
    "        for subdirname in dirnames:\n",
    "            recursively_process_files(subdirname, processing_func)\n",
    " \n",
    "        for filename in filenames:\n",
    "            processing_func(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "recursively_process_files(f\"{dataset_dir}\", resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LST files\n",
      "Label classes:\n",
      "BACKGROUND_Google 0\n",
      "Faces 1\n",
      "Faces_easy 2\n",
      "Leopards 3\n",
      "Motorbikes 4\n",
      "accordion 5\n",
      "airplanes 6\n",
      "anchor 7\n",
      "ant 8\n",
      "barrel 9\n",
      "bass 10\n",
      "beaver 11\n",
      "binocular 12\n",
      "bonsai 13\n",
      "brain 14\n",
      "brontosaurus 15\n",
      "buddha 16\n",
      "butterfly 17\n",
      "camera 18\n",
      "cannon 19\n",
      "car_side 20\n",
      "ceiling_fan 21\n",
      "cellphone 22\n",
      "chair 23\n",
      "chandelier 24\n",
      "cougar_body 25\n",
      "cougar_face 26\n",
      "crab 27\n",
      "crayfish 28\n",
      "crocodile 29\n",
      "crocodile_head 30\n",
      "cup 31\n",
      "dalmatian 32\n",
      "dollar_bill 33\n",
      "dolphin 34\n",
      "dragonfly 35\n",
      "electric_guitar 36\n",
      "elephant 37\n",
      "emu 38\n",
      "euphonium 39\n",
      "ewer 40\n",
      "ferry 41\n",
      "flamingo 42\n",
      "flamingo_head 43\n",
      "garfield 44\n",
      "gerenuk 45\n",
      "gramophone 46\n",
      "grand_piano 47\n",
      "hawksbill 48\n",
      "headphone 49\n",
      "hedgehog 50\n",
      "helicopter 51\n",
      "ibis 52\n",
      "inline_skate 53\n",
      "joshua_tree 54\n",
      "kangaroo 55\n",
      "ketch 56\n",
      "lamp 57\n",
      "laptop 58\n",
      "llama 59\n",
      "lobster 60\n",
      "lotus 61\n",
      "mandolin 62\n",
      "mayfly 63\n",
      "menorah 64\n",
      "metronome 65\n",
      "minaret 66\n",
      "nautilus 67\n",
      "octopus 68\n",
      "okapi 69\n",
      "pagoda 70\n",
      "panda 71\n",
      "pigeon 72\n",
      "pizza 73\n",
      "platypus 74\n",
      "pyramid 75\n",
      "revolver 76\n",
      "rhino 77\n",
      "rooster 78\n",
      "saxophone 79\n",
      "schooner 80\n",
      "scissors 81\n",
      "scorpion 82\n",
      "sea_horse 83\n",
      "snoopy 84\n",
      "soccer_ball 85\n",
      "stapler 86\n",
      "starfish 87\n",
      "stegosaurus 88\n",
      "stop_sign 89\n",
      "strawberry 90\n",
      "sunflower 91\n",
      "tick 92\n",
      "trilobite 93\n",
      "umbrella 94\n",
      "watch 95\n",
      "water_lilly 96\n",
      "wheelchair 97\n",
      "wild_cat 98\n",
      "windsor_chair 99\n",
      "wrench 100\n",
      "yin_yang 101\n",
      "Creating RecordIO files\n",
      "Creating .rec file from /tmp/caltech_objects_train.lst in /tmp\n",
      "time: 0.037541866302490234  count: 0\n",
      "time: 1.772956132888794  count: 1000\n",
      "time: 1.713017463684082  count: 2000\n",
      "time: 1.7182867527008057  count: 3000\n",
      "time: 1.7370080947875977  count: 4000\n",
      "time: 1.6987528800964355  count: 5000\n",
      "time: 1.7218694686889648  count: 6000\n",
      "Creating .rec file from /tmp/caltech_objects_test.lst in /tmp\n",
      "time: 0.0262453556060791  count: 0\n",
      "time: 1.7814793586730957  count: 1000\n",
      "time: 1.7351763248443604  count: 2000\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  47M Nov 21 04:08 caltech_objects_test.rec\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 108M Nov 21 04:08 caltech_objects_train.rec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘caltech_objects_classes’: No such file or directory\n",
      "rm: cannot remove ‘*.rec’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use the IM2REC script to convert our images into RecordIO files\n",
    "\n",
    "cd $BASE_DIR\n",
    "\n",
    "rm ${DATASET_NAME}_classes\n",
    "\n",
    "# First we need to create two LST files (training and test lists), noting the correct label class for each image\n",
    "# We'll also save the output of the LST files command, since it includes a list of all of our label classes\n",
    "echo \"Creating LST files\"\n",
    "python $IM2REC --list --recursive --pass-through --test-ratio=0.3 --train-ratio=0.7 $DATASET_NAME $DATASET_NAME > ${DATASET_NAME}_classes\n",
    "\n",
    "# Then we create RecordIO files from the LST files\n",
    "echo \"Creating RecordIO files\"\n",
    "rm *.rec\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_train.lst $DATASET_NAME\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_test.lst $DATASET_NAME\n",
    "ls -lh *.rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cut:',\n",
       " '{base_dir}/{classes_file_name}:',\n",
       " 'No',\n",
       " 'such',\n",
       " 'file',\n",
       " 'or',\n",
       " 'directory']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of class labels into a python variable to use later\n",
    "class_labels = ! cut -d \" \" -f1 {base_dir}/{classes_file_name} | paste -sd \" \"\n",
    "class_labels = class_labels[0].split(' ')\n",
    "\n",
    "# Show a few of them now just so we can see the kind of labels we're working with...\n",
    "class_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../tmp/caltech_objects_train.rec to s3://sagemaker-us-west-2-541003905521/caltech_objects/train/caltech_objects_train.rec\n",
      "upload: ../../../tmp/caltech_objects_test.rec to s3://sagemaker-us-west-2-541003905521/caltech_objects/validation/caltech_objects_test.rec\n"
     ]
    }
   ],
   "source": [
    "# Upload our train and test RecordIO files to S3 in the bucket that our sagemaker session is using\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "s3train_path = 's3://{}/{}/train/'.format(bucket, dataset_name)\n",
    "s3validation_path = 's3://{}/{}/validation/'.format(bucket, dataset_name)\n",
    "\n",
    "# Clean up any existing data in our training s3 bucket\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/train --recursive\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/validation --recursive\n",
    "\n",
    "# Upload the rec files to the train and validation channels\n",
    "!aws s3 cp /tmp/{dataset_name}_train.rec $s3train_path\n",
    "!aws s3 cp /tmp/{dataset_name}_test.rec $s3validation_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the data for our model training to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3train_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3validation_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, dataset_name)\n",
    "image_classifier = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_wait=60*60,\n",
    "    train_max_run=60*60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_pretrained_model': 1,\n",
       " 'image_shape': '3,224,224',\n",
       " 'num_classes': 102,\n",
       " 'num_training_samples': 6400,\n",
       " 'epochs': 60,\n",
       " 'learning_rate': 0.001,\n",
       " 'mini_batch_size': 64,\n",
       " 'early_stopping': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes=! ls -l {base_dir}/{dataset_name} | wc -l\n",
    "num_classes=int(num_classes[0]) - 1\n",
    "\n",
    "num_training_samples=! cat {base_dir}/{dataset_name}_train.lst | wc -l\n",
    "num_training_samples = int(num_training_samples[0])\n",
    "\n",
    "# Learn more about the Sagemaker built-in Image Classifier hyperparameters here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "# These hyperparameters we won't want to change, as they define things like\n",
    "# the size of the images we'll be sending for input, the number of training classes we have, etc.\n",
    "base_hyperparameters=dict(\n",
    "    use_pretrained_model=1,\n",
    "    image_shape='3,224,224',\n",
    "    num_classes=num_classes,\n",
    "    num_training_samples=num_training_samples,\n",
    ")\n",
    "\n",
    "# These are hyperparameters we may want to tune, as they can affect the model training success:\n",
    "hyperparameters={\n",
    "    **base_hyperparameters, \n",
    "    **dict(\n",
    "        epochs=60,\n",
    "        learning_rate=0.001,\n",
    "        mini_batch_size=64,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "image_classifier.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "This will take some time because it's provisioning a new container runtime to train our model, then the actual training happens, then the trained model gets uploaded to S3 and the container is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-21 04:08:48 Starting - Starting the training job...\n",
      "2019-11-21 04:08:50 Starting - Launching requested ML instances......\n",
      "2019-11-21 04:10:18 Starting - Preparing the instances for training.........\n",
      "2019-11-21 04:11:34 Downloading - Downloading input data\n",
      "2019-11-21 04:11:34 Training - Downloading the training image.....\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'use_pretrained_model': u'1', u'epochs': u'60', u'num_training_samples': u'6400', u'num_classes': u'102', u'mini_batch_size': u'64', u'image_shape': u'3,224,224', u'early_stopping': u'True'}\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.001', u'epochs': u'60', u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'precision_dtype': u'float32', u'mini_batch_size': u'64', u'early_stopping': u'True', u'num_classes': u'102', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'6400'}\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] multi_label: 0\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] ---- Parameters ----\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] num_layers: 152\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] epochs: 60\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] optimizer: sgd\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] momentum: 0.9\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] learning_rate: 0.001\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] num_training_samples: 6400\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] mini_batch_size: 64\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] num_classes: 102\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] augmentation_type: None\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] kv_store: device\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Using early stopping for training\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Early stopping minimum epochs: 10\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Early stopping patience: 10\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] Early stopping tolerance: 0.01\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:26 INFO 140138295818048] --------------------\u001b[0m\n",
      "\u001b[31m[04:12:26] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[31m[04:12:26] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:27 INFO 140138295818048] Setting number of threads: 7\u001b[0m\n",
      "\n",
      "2019-11-21 04:12:23 Training - Training image download completed. Training in progress.\u001b[31m[04:12:35] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:47 INFO 140138295818048] Epoch[0] Batch [20]#011Speed: 107.493 samples/sec#011accuracy=0.155506\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:12:56 INFO 140138295818048] Epoch[0] Batch [40]#011Speed: 123.153 samples/sec#011accuracy=0.247713\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:04 INFO 140138295818048] Epoch[0] Batch [60]#011Speed: 129.385 samples/sec#011accuracy=0.304047\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:13 INFO 140138295818048] Epoch[0] Batch [80]#011Speed: 132.832 samples/sec#011accuracy=0.354552\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:22 INFO 140138295818048] Epoch[0] Train-accuracy=0.393281\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:22 INFO 140138295818048] Epoch[0] Time cost=46.965\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:28 INFO 140138295818048] Epoch[0] Validation-accuracy=0.614462\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:29 INFO 140138295818048] Storing the best model with validation accuracy: 0.614462\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:29 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:38 INFO 140138295818048] Epoch[1] Batch [20]#011Speed: 138.499 samples/sec#011accuracy=0.665923\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:47 INFO 140138295818048] Epoch[1] Batch [40]#011Speed: 141.347 samples/sec#011accuracy=0.697790\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:13:56 INFO 140138295818048] Epoch[1] Batch [60]#011Speed: 142.149 samples/sec#011accuracy=0.728484\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:05 INFO 140138295818048] Epoch[1] Batch [80]#011Speed: 142.509 samples/sec#011accuracy=0.749807\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:13 INFO 140138295818048] Epoch[1] Train-accuracy=0.769844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:13 INFO 140138295818048] Epoch[1] Time cost=44.331\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:20 INFO 140138295818048] Epoch[1] Validation-accuracy=0.841570\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:20 INFO 140138295818048] Storing the best model with validation accuracy: 0.841570\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:20 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:30 INFO 140138295818048] Epoch[2] Batch [20]#011Speed: 138.272 samples/sec#011accuracy=0.901042\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:39 INFO 140138295818048] Epoch[2] Batch [40]#011Speed: 141.300 samples/sec#011accuracy=0.907012\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:47 INFO 140138295818048] Epoch[2] Batch [60]#011Speed: 142.205 samples/sec#011accuracy=0.915471\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:14:56 INFO 140138295818048] Epoch[2] Batch [80]#011Speed: 142.396 samples/sec#011accuracy=0.921103\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:05 INFO 140138295818048] Epoch[2] Train-accuracy=0.929844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:05 INFO 140138295818048] Epoch[2] Time cost=44.363\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:11 INFO 140138295818048] Epoch[2] Validation-accuracy=0.919331\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:12 INFO 140138295818048] Storing the best model with validation accuracy: 0.919331\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:12 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:21 INFO 140138295818048] Epoch[3] Batch [20]#011Speed: 138.279 samples/sec#011accuracy=0.964286\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:30 INFO 140138295818048] Epoch[3] Batch [40]#011Speed: 141.119 samples/sec#011accuracy=0.969893\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:39 INFO 140138295818048] Epoch[3] Batch [60]#011Speed: 142.198 samples/sec#011accuracy=0.972336\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:48 INFO 140138295818048] Epoch[3] Batch [80]#011Speed: 142.673 samples/sec#011accuracy=0.972608\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:56 INFO 140138295818048] Epoch[3] Train-accuracy=0.974844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:15:56 INFO 140138295818048] Epoch[3] Time cost=44.295\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:03 INFO 140138295818048] Epoch[3] Validation-accuracy=0.938590\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:03 INFO 140138295818048] Storing the best model with validation accuracy: 0.938590\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:03 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0004.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:13 INFO 140138295818048] Epoch[4] Batch [20]#011Speed: 138.327 samples/sec#011accuracy=0.985863\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:22 INFO 140138295818048] Epoch[4] Batch [40]#011Speed: 141.309 samples/sec#011accuracy=0.986662\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:30 INFO 140138295818048] Epoch[4] Batch [60]#011Speed: 142.278 samples/sec#011accuracy=0.988217\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:39 INFO 140138295818048] Epoch[4] Batch [80]#011Speed: 142.571 samples/sec#011accuracy=0.987269\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:48 INFO 140138295818048] Epoch[4] Train-accuracy=0.988125\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:48 INFO 140138295818048] Epoch[4] Time cost=44.318\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:54 INFO 140138295818048] Epoch[4] Validation-accuracy=0.941134\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:55 INFO 140138295818048] Storing the best model with validation accuracy: 0.941134\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:16:55 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:04 INFO 140138295818048] Epoch[5] Batch [20]#011Speed: 138.385 samples/sec#011accuracy=0.995536\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:13 INFO 140138295818048] Epoch[5] Batch [40]#011Speed: 141.363 samples/sec#011accuracy=0.993140\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:22 INFO 140138295818048] Epoch[5] Batch [60]#011Speed: 142.363 samples/sec#011accuracy=0.993340\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:31 INFO 140138295818048] Epoch[5] Batch [80]#011Speed: 142.646 samples/sec#011accuracy=0.993827\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:39 INFO 140138295818048] Epoch[5] Train-accuracy=0.994062\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:39 INFO 140138295818048] Epoch[5] Time cost=44.290\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:46 INFO 140138295818048] Epoch[5] Validation-accuracy=0.942224\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:46 INFO 140138295818048] Storing the best model with validation accuracy: 0.942224\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:46 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0006.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:17:56 INFO 140138295818048] Epoch[6] Batch [20]#011Speed: 138.209 samples/sec#011accuracy=0.997768\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:04 INFO 140138295818048] Epoch[6] Batch [40]#011Speed: 141.212 samples/sec#011accuracy=0.996570\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:13 INFO 140138295818048] Epoch[6] Batch [60]#011Speed: 142.163 samples/sec#011accuracy=0.997182\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:22 INFO 140138295818048] Epoch[6] Batch [80]#011Speed: 142.577 samples/sec#011accuracy=0.997106\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:31 INFO 140138295818048] Epoch[6] Train-accuracy=0.997344\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:31 INFO 140138295818048] Epoch[6] Time cost=44.315\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:37 INFO 140138295818048] Epoch[6] Validation-accuracy=0.945494\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:38 INFO 140138295818048] Storing the best model with validation accuracy: 0.945494\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:38 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0007.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:47 INFO 140138295818048] Epoch[7] Batch [20]#011Speed: 138.190 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:18:56 INFO 140138295818048] Epoch[7] Batch [40]#011Speed: 141.295 samples/sec#011accuracy=0.998476\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:05 INFO 140138295818048] Epoch[7] Batch [60]#011Speed: 142.111 samples/sec#011accuracy=0.998719\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:14 INFO 140138295818048] Epoch[7] Batch [80]#011Speed: 142.434 samples/sec#011accuracy=0.998650\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:22 INFO 140138295818048] Epoch[7] Train-accuracy=0.998594\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:22 INFO 140138295818048] Epoch[7] Time cost=44.343\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:29 INFO 140138295818048] Epoch[7] Validation-accuracy=0.946801\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:29 INFO 140138295818048] Storing the best model with validation accuracy: 0.946801\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:29 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0008.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:38 INFO 140138295818048] Epoch[8] Batch [20]#011Speed: 138.268 samples/sec#011accuracy=0.999256\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:47 INFO 140138295818048] Epoch[8] Batch [40]#011Speed: 141.205 samples/sec#011accuracy=0.999238\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:19:56 INFO 140138295818048] Epoch[8] Batch [60]#011Speed: 142.201 samples/sec#011accuracy=0.998975\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:05 INFO 140138295818048] Epoch[8] Batch [80]#011Speed: 142.510 samples/sec#011accuracy=0.998650\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:14 INFO 140138295818048] Epoch[8] Train-accuracy=0.998750\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:14 INFO 140138295818048] Epoch[8] Time cost=44.328\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:20 INFO 140138295818048] Epoch[8] Validation-accuracy=0.947311\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:20 INFO 140138295818048] Storing the best model with validation accuracy: 0.947311\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:21 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0009.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:30 INFO 140138295818048] Epoch[9] Batch [20]#011Speed: 137.637 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:39 INFO 140138295818048] Epoch[9] Batch [40]#011Speed: 140.973 samples/sec#011accuracy=0.999619\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:48 INFO 140138295818048] Epoch[9] Batch [60]#011Speed: 141.977 samples/sec#011accuracy=0.999744\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:20:57 INFO 140138295818048] Epoch[9] Batch [80]#011Speed: 142.333 samples/sec#011accuracy=0.999807\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:05 INFO 140138295818048] Epoch[9] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:05 INFO 140138295818048] Epoch[9] Time cost=44.378\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:12 INFO 140138295818048] Epoch[9] Validation-accuracy=0.948038\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:12 INFO 140138295818048] Storing the best model with validation accuracy: 0.948038\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:12 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0010.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:22 INFO 140138295818048] Epoch[10] Batch [20]#011Speed: 138.501 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:30 INFO 140138295818048] Epoch[10] Batch [40]#011Speed: 141.402 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:39 INFO 140138295818048] Epoch[10] Batch [60]#011Speed: 142.402 samples/sec#011accuracy=0.999744\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:48 INFO 140138295818048] Epoch[10] Batch [80]#011Speed: 142.643 samples/sec#011accuracy=0.999807\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:57 INFO 140138295818048] Epoch[10] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:21:57 INFO 140138295818048] Epoch[10] Time cost=44.299\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:03 INFO 140138295818048] Epoch[10] Validation-accuracy=0.949855\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:03 INFO 140138295818048] Storing the best model with validation accuracy: 0.949855\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:04 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0011.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:13 INFO 140138295818048] Epoch[11] Batch [20]#011Speed: 138.819 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:22 INFO 140138295818048] Epoch[11] Batch [40]#011Speed: 141.566 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:31 INFO 140138295818048] Epoch[11] Batch [60]#011Speed: 142.569 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:40 INFO 140138295818048] Epoch[11] Batch [80]#011Speed: 143.017 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:48 INFO 140138295818048] Epoch[11] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:48 INFO 140138295818048] Epoch[11] Time cost=44.205\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:22:54 INFO 140138295818048] Epoch[11] Validation-accuracy=0.947674\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:04 INFO 140138295818048] Epoch[12] Batch [20]#011Speed: 138.251 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:13 INFO 140138295818048] Epoch[12] Batch [40]#011Speed: 141.414 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:22 INFO 140138295818048] Epoch[12] Batch [60]#011Speed: 142.399 samples/sec#011accuracy=0.999744\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:31 INFO 140138295818048] Epoch[12] Batch [80]#011Speed: 142.854 samples/sec#011accuracy=0.999807\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:39 INFO 140138295818048] Epoch[12] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:39 INFO 140138295818048] Epoch[12] Time cost=44.234\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:46 INFO 140138295818048] Epoch[12] Validation-accuracy=0.949491\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:23:55 INFO 140138295818048] Epoch[13] Batch [20]#011Speed: 138.457 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:04 INFO 140138295818048] Epoch[13] Batch [40]#011Speed: 141.335 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:13 INFO 140138295818048] Epoch[13] Batch [60]#011Speed: 142.332 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:22 INFO 140138295818048] Epoch[13] Batch [80]#011Speed: 142.839 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:30 INFO 140138295818048] Epoch[13] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:30 INFO 140138295818048] Epoch[13] Time cost=44.236\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:37 INFO 140138295818048] Epoch[13] Validation-accuracy=0.950945\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:37 INFO 140138295818048] Storing the best model with validation accuracy: 0.950945\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:38 INFO 140138295818048] Saved checkpoint to \"/opt/ml/model/image-classification-0014.params\"\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:47 INFO 140138295818048] Epoch[14] Batch [20]#011Speed: 138.573 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:24:56 INFO 140138295818048] Epoch[14] Batch [40]#011Speed: 141.421 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:04 INFO 140138295818048] Epoch[14] Batch [60]#011Speed: 142.424 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:13 INFO 140138295818048] Epoch[14] Batch [80]#011Speed: 142.790 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:22 INFO 140138295818048] Epoch[14] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:22 INFO 140138295818048] Epoch[14] Time cost=44.249\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:28 INFO 140138295818048] Epoch[14] Validation-accuracy=0.948289\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:38 INFO 140138295818048] Epoch[15] Batch [20]#011Speed: 138.260 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:47 INFO 140138295818048] Epoch[15] Batch [40]#011Speed: 141.320 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:25:56 INFO 140138295818048] Epoch[15] Batch [60]#011Speed: 142.319 samples/sec#011accuracy=0.999744\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:04 INFO 140138295818048] Epoch[15] Batch [80]#011Speed: 142.854 samples/sec#011accuracy=0.999807\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:13 INFO 140138295818048] Epoch[15] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:13 INFO 140138295818048] Epoch[15] Time cost=44.217\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:19 INFO 140138295818048] Epoch[15] Validation-accuracy=0.950581\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:29 INFO 140138295818048] Epoch[16] Batch [20]#011Speed: 138.695 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:38 INFO 140138295818048] Epoch[16] Batch [40]#011Speed: 141.627 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:47 INFO 140138295818048] Epoch[16] Batch [60]#011Speed: 142.626 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:26:56 INFO 140138295818048] Epoch[16] Batch [80]#011Speed: 143.135 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:04 INFO 140138295818048] Epoch[16] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:04 INFO 140138295818048] Epoch[16] Time cost=44.162\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:10 INFO 140138295818048] Epoch[16] Validation-accuracy=0.948765\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:20 INFO 140138295818048] Epoch[17] Batch [20]#011Speed: 138.791 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:29 INFO 140138295818048] Epoch[17] Batch [40]#011Speed: 141.678 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:38 INFO 140138295818048] Epoch[17] Batch [60]#011Speed: 142.694 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:47 INFO 140138295818048] Epoch[17] Batch [80]#011Speed: 143.157 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:55 INFO 140138295818048] Epoch[17] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:27:55 INFO 140138295818048] Epoch[17] Time cost=44.148\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:02 INFO 140138295818048] Epoch[17] Validation-accuracy=0.950945\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:11 INFO 140138295818048] Epoch[18] Batch [20]#011Speed: 138.675 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:20 INFO 140138295818048] Epoch[18] Batch [40]#011Speed: 141.755 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:29 INFO 140138295818048] Epoch[18] Batch [60]#011Speed: 142.645 samples/sec#011accuracy=0.999744\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:38 INFO 140138295818048] Epoch[18] Batch [80]#011Speed: 143.080 samples/sec#011accuracy=0.999807\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:46 INFO 140138295818048] Epoch[18] Train-accuracy=0.999687\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:46 INFO 140138295818048] Epoch[18] Time cost=44.158\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:28:53 INFO 140138295818048] Epoch[18] Validation-accuracy=0.950218\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:02 INFO 140138295818048] Epoch[19] Batch [20]#011Speed: 138.812 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:11 INFO 140138295818048] Epoch[19] Batch [40]#011Speed: 141.773 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:20 INFO 140138295818048] Epoch[19] Batch [60]#011Speed: 142.750 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:29 INFO 140138295818048] Epoch[19] Batch [80]#011Speed: 143.213 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:37 INFO 140138295818048] Epoch[19] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:37 INFO 140138295818048] Epoch[19] Time cost=44.119\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:44 INFO 140138295818048] Epoch[19] Validation-accuracy=0.950218\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:29:53 INFO 140138295818048] Epoch[20] Batch [20]#011Speed: 138.720 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:30:02 INFO 140138295818048] Epoch[20] Batch [40]#011Speed: 141.753 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:30:11 INFO 140138295818048] Epoch[20] Batch [60]#011Speed: 142.645 samples/sec#011accuracy=0.999744\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:30:20 INFO 140138295818048] Epoch[20] Batch [80]#011Speed: 143.156 samples/sec#011accuracy=0.999807\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:30:28 INFO 140138295818048] Epoch[20] Train-accuracy=0.999844\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:30:28 INFO 140138295818048] Epoch[20] Time cost=44.141\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:30:35 INFO 140138295818048] Epoch[20] Validation-accuracy=0.950218\u001b[0m\n",
      "\u001b[31m[11/21/2019 04:30:35 INFO 140138295818048] Early stopping criteria met.\u001b[0m\n",
      "\n",
      "2019-11-21 04:30:41 Uploading - Uploading generated training model\n",
      "2019-11-21 04:31:12 Completed - Training job completed\n",
      "Training seconds: 1189\n",
      "Billable seconds: 357\n",
      "Managed Spot Training savings: 70.0%\n",
      "\n",
      "\n",
      " Finished training! The model is available for download at: s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-21-04-08-48-253/output/model.tar.gz\n",
      "CPU times: user 2.97 s, sys: 121 ms, total: 3.09 s\n",
      "Wall time: 22min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "image_classifier.fit(inputs=data_channels, logs=True)\n",
    "\n",
    "job = image_classifier.latest_training_job\n",
    "model_path = f\"{base_dir}/{job.name}\"\n",
    "\n",
    "print(f\"\\n\\n Finished training! The model is available for download at: {image_classifier.output_path}/{job.name}/output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting our SageMaker-trained model to the ONNX Format\n",
    "\n",
    "SageMaker uses a framework called MXNet to train and produce our image classifier model.  But, we might want to use this model to perform inference somehwere that MXNet doesn't easily run, such as in a web browser.  \n",
    "\n",
    "ONNX is an open format to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools and choose the combination that is best for them. More info at https://onnx.ai/\n",
    "\n",
    "SageMaker provides helpful tooling in the Python SDK to convert trained models to the ONNX format, making it easy to take your trained model, convert it to ONNX, then use that model in whatever environment you want to (as long as that environment will accept models in the ONNX format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_S3_PATH=s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-21-04-08-48-253/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Get the path where our trained model was saved to S3\n",
    "model_s3_path = image_classifier.model_data\n",
    "model_s3_output_dir = \"/\".join(model_s3_path.split('/')[0:-1])\n",
    "%env MODEL_S3_PATH = $model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-21-04-08-48-253/output/model.tar.gz to ../../../tmp/downloaded_model/model.tar.gz\n",
      "model-shapes.json\n",
      "image-classification-symbol.json\n",
      "image-classification-0014.params\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download our model and extract it into $BASE_DIR/downloaded_model\n",
    "rm -rf $BASE_DIR/downloaded_model/\n",
    "mkdir -p $BASE_DIR/downloaded_model/\n",
    "aws s3 cp $MODEL_S3_PATH $BASE_DIR/downloaded_model/model.tar.gz\n",
    "cd $BASE_DIR/downloaded_model\n",
    "tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../tmp/downloaded_model/model.onnx to s3://sagemaker-us-west-2-541003905521/caltech_objects/output/image-classification-2019-11-21-04-08-48-253/output/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Use MXNet's onnx_mxnet module to convert the MXNet model that SageMaker trained into ONNX format\n",
    "from mxnet.contrib import onnx as onnx_mxnet\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "model_dir = base_dir + \"/downloaded_model\"\n",
    "\n",
    "model_symbol_file = glob(model_dir + \"/*symbol.json\")[0]\n",
    "model_params_file = glob(model_dir + \"/*.params\")[0]\n",
    "\n",
    "onnx_mxnet.export_model(sym=model_symbol_file,\n",
    "                            params=model_params_file,\n",
    "                            input_shape=[(1, 3, 224, 224)],\n",
    "                            input_type=np.float32,\n",
    "                            onnx_file_path=\"{}/model.onnx\".format(model_dir),\n",
    "                            verbose=True)\n",
    "\n",
    "# And upload the ONNX model back to the same place on S3 as where we Sagemaker put the MXNet version of the model, just for safe-keeping\n",
    "! aws s3 cp {model_dir}/model.onnx {model_s3_output_dir}/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Click here to download the model in ONNX format: <a href='model.onnx' download=model.onnx>model.onnx</a><br>"
      ],
      "text/plain": [
       "/home/ec2-user/SageMaker/model.onnx"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, display a link so we can download the ONNX version of the model easily from this notebook's local disk\n",
    "\n",
    "from IPython.display import FileLink\n",
    "# DownloadFileLink via https://github.com/jupyterlab/jupyterlab/issues/5443\n",
    "class DownloadFileLink(FileLink):\n",
    "    html_link_str = \"<a href='{link}' download={file_name}>{link_text}</a>\"\n",
    "\n",
    "    def __init__(self, path, file_name=None, link_text=None, *args, **kwargs):\n",
    "        super(DownloadFileLink, self).__init__(path, *args, **kwargs)\n",
    "\n",
    "        self.file_name = file_name or os.path.split(path)[1]\n",
    "        self.link_text = link_text or self.file_name\n",
    "\n",
    "    def _format_path(self):\n",
    "        from html import escape\n",
    "        fp = ''.join([self.url_prefix, escape(self.path)])\n",
    "        return ''.join([self.result_html_prefix,\n",
    "                        self.html_link_str.format(link=fp, file_name=self.file_name, link_text=self.link_text),\n",
    "                        self.result_html_suffix])\n",
    "    \n",
    "# We'll need to symlink the onnx model file from base_dir/downloaded_model/model.onnx to this notebook's home directory \n",
    "# so that Jupyter will serve it\n",
    "! ln -fs {model_dir}/model.onnx ./model.onnx\n",
    "\n",
    "# Output the download link for us to click on\n",
    "DownloadFileLink(\"model.onnx\", result_html_prefix=\"Click here to download the model in ONNX format: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKGROUND_Google Faces Faces_easy Leopards Motorbikes accordion airplanes anchor ant barrel bass beaver binocular bonsai brain brontosaurus buddha butterfly camera cannon car_side ceiling_fan cellphone chair chandelier cougar_body cougar_face crab crayfish crocodile crocodile_head cup dalmatian dollar_bill dolphin dragonfly electric_guitar elephant emu euphonium ewer ferry flamingo flamingo_head garfield gerenuk gramophone grand_piano hawksbill headphone hedgehog helicopter ibis inline_skate joshua_tree kangaroo ketch lamp laptop llama lobster lotus mandolin mayfly menorah metronome minaret nautilus octopus okapi pagoda panda pigeon pizza platypus pyramid revolver rhino rooster saxophone schooner scissors scorpion sea_horse snoopy soccer_ball stapler starfish stegosaurus stop_sign strawberry sunflower tick trilobite umbrella watch water_lilly wheelchair wild_cat windsor_chair wrench yin_yang\n"
     ]
    }
   ],
   "source": [
    "# Finally, we'll also want the handy list of knowing all the classes that our model returns scores for. \n",
    "# Our model returns a score for each label, in this same order.\n",
    "\n",
    "print(' '.join(class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ONNX Model for inference\n",
    "\n",
    "Now that you have an ONNX format of your image classifer model downloaded, \n",
    "you can use that model, along with the list of class labels (shown above),\n",
    "to make inferences about images any place that can run ONNX models, including \n",
    "offline, in your web browser! \n",
    "\n",
    "Want to give that a try? Download your ONNX model, copy the class labels list above,\n",
    "then visit this URL which will load your model into your web browser (it doesn't upload the model anywhere)\n",
    "and make inferences on images that you drag and drop onto the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Using SageMaker to host a deployed version of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: image-classification-2019-11-15-03-41-36-403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------!CPU times: user 691 ms, sys: 24.3 ms, total: 715 ms\n",
      "Wall time: 11min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Deploying a model to an endpoint takes a few minutes to complete\n",
    "\n",
    "assert False # Change this to True to deploy. This takes about 10 minutes to complete this step.\n",
    "\n",
    "import time\n",
    "now = str(int(time.time()))\n",
    "\n",
    "deployed_endpoint = image_classifier.deploy(\n",
    "    endpoint_name = dataset_name.replace('_', '-') + '-' + now,\n",
    "    #endpoint_name = \"stitches-1573714918\",\n",
    "    #update_endpoint = True,\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.t2.medium',\n",
    ")\n",
    "\n",
    "# image-classification-2019-02-01-06-47-08-571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling a deployed endpoint from Python code\n",
    "\n",
    "Once you've deployed to a SakeMaker hosted endpoint, you'll want to pass it some images to see it perform inferences.  Here's how to do this from some python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def classify_deployed(file_name, classes):\n",
    "    payload = None\n",
    "    with open(file_name, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    deployed_endpoint.content_type = 'application/x-image'\n",
    "    result = json.loads(deployed_endpoint.predict(payload))\n",
    "    print(result)\n",
    "    best_prob_index = np.argmax(result)\n",
    "    return (classes[best_prob_index], result[best_prob_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_url=\"https://some/image/to/download\"\n",
    "\n",
    "# Download an image from a URL\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# And resize it\n",
    "size = 224\n",
    "new_im = Image.new('RGB', (size, size), (0, 0, 0, 0))\n",
    "img.thumbnail([size, size])\n",
    "(w, h) = img.size\n",
    "new_im.paste(img, (int((size - w) / 2), int((size - h) / 2 )))\n",
    "new_im.save(tmp_local_path, \"JPEG\", quality = 95)\n",
    "\n",
    "# Show it\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=tmp_local_path))\n",
    "\n",
    "# Classify it\n",
    "classify_deployed(tmp_local_path, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Perform Hyperparameter Tuning\n",
    "\n",
    "Often, you might not know which values for hyperparameters like `learning_rate` and `mini_batch_size` will yield acceptible results. Traditionally, this meant manually running many training jobs with different hyperparameter values, looking at each trained model's performance, and then picking a winner. \n",
    "\n",
    "\n",
    "This type of manual tuning is _very_ time consuming, so you can automate this process using automatic model tuning with SageMaker. Here's some example code to illustrate how to start one of these jobs using the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "\n",
    "assert False # Change to True to run this\n",
    "\n",
    "# ---------------\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "hyperparameter_ranges = {\n",
    "    'optimizer': CategoricalParameter(['sgd', 'adam', 'rmsprop', 'nag']),\n",
    "     'learning_rate': ContinuousParameter(0.0001, 0.1),\n",
    "     'mini_batch_size': CategoricalParameter([4, 8, 16, 32]),\n",
    "     'momentum': ContinuousParameter(0.0, 0.99),                                                                   \n",
    "     'weight_decay': ContinuousParameter(0.0, 0.99),   \n",
    "}\n",
    "\n",
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    image_classifier,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    early_stopping_type='Off',\n",
    "    max_jobs=500,\n",
    "    max_parallel_jobs=2\n",
    ")\n",
    "\n",
    "\n",
    "tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "When you're done learning here, you'll likley want to delete the endpoint you deployed (if you did so), since that endpoint will charge you money even if you're not actively making inferences (because you're charged for the total availability time of the endpoint). Here's how to clean up the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_endpoint.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
